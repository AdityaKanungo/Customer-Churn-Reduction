rm(list=ls())
setwd("C:/Users/BATMAN/Desktop/1st project working copy")
getwd()
#------------------------------
library(corrplot)
library(DMwR)
library(e1071)
library(caret)
library(class)
library(C50)
#------------------------------
# Load train data
train_df =  read.csv("C:/Users/BATMAN/Desktop/1st project working copy/Train_data.csv")
test_df =  read.csv("C:/Users/BATMAN/Desktop/1st project working copy/Test_data.csv")
final_submission = test_df
#------------------------------
#remove un-wanted variables
train_df <- subset(train_df, select = -c(state,area.code,phone.number))
test_df <- subset(test_df, select = -c(state,area.code,phone.number))
#------------------------------
#Replacing yes and no with 1 and 0/ true false with 1 and 0
#train
train_df$international.plan = as.numeric(as.factor(train_df$international.plan)==" yes",0,1)
train_df$voice.mail.plan = as.numeric(as.factor(train_df$voice.mail.plan)==" yes",0,1)
train_df$Churn = as.numeric(as.factor(train_df$Churn)==" True.",0,1)
#test
test_df$international.plan = as.numeric(as.factor(test_df$international.plan)==" yes",0,1)
test_df$voice.mail.plan = as.numeric(as.factor(test_df$voice.mail.plan)==" yes",0,1)
test_df$Churn = as.numeric(as.factor(test_df$Churn)==" True.",0,1)
#Converting into factor
train_df$Churn = as.factor(train_df$Churn)
train_df$international.plan = as.factor(train_df$international.plan)
train_df$voice.mail.plan = as.factor(train_df$voice.mail.plan)
test_df$Churn = as.factor(test_df$Churn)
test_df$international.plan = as.factor(test_df$international.plan)
test_df$voice.mail.plan = as.factor(test_df$voice.mail.plan)
#------------------------------
#Check missing vales
sum(is.na(train_df))
sum(is.na(test_df))
#------------------------------
## feature selection
#------------------------------
#correlation plot
w = train_df[,5:16]
x = cor(w)
corrplot(x, type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)
#From the above heat-map we can infer the following:
#- total day minutes & total day charge are highly +vely correlated.
#- total eve minutes & total eve charge are highly +vely correlated.
#- total night minutes & total night charge are highly +vely correlated.
#- total intl minutes & total intl charge are highly +vely correlated.
#Therefore we will drop the total day charge, total eve charge, total night charge i.e variables carrying redundant information
train_df <- subset(train_df, select = -c(total.day.minutes,total.eve.minutes,total.night.minutes,total.intl.minutes))
test_df <- subset(test_df, select = -c(total.day.minutes,total.eve.minutes,total.night.minutes,total.intl.minutes))
#------------------------------
## Outlier analysis using boxplot
#------------------------------
numeric_index = sapply(train_df,is.numeric) #selecting only numeric
numeric_data = train_df[,numeric_index]
cnames = colnames(numeric_data)
#loop to plot box-plot for each variable
for(i in cnames)
{
boxplot(train_df[i])
}
#loop to remove all outliers
#train
for(i in cnames)
train_df = knnImputation(train_df, k=3)
{
val = train_df[,i][train_df[,i] %in% boxplot.stats(train_df[,i])$out]
train_df[,i][train_df[,i] %in% val] = NA
}
numeric_index = sapply(test_df,is.numeric) #selecting only numeric
numeric_data = test_df[,numeric_index]
numeric_data <- subset(numeric_data, select = -c(number.vmail.messages))
cnames = colnames(numeric_data)
#test
for(i in cnames)
{
val = test_df[,i][test_df[,i] %in% boxplot.stats(test_df[,i])$out]
test_df[,i][test_df[,i] %in% val] = NA
}
test_df = knnImputation(test_df, k=3)
#------------------------------
## Feature scaling
#------------------------------
#standardization
#train
numeric_index = sapply(train_df,is.numeric) #selecting only numeric
numeric_data = train_df[,numeric_index]
cnames = colnames(numeric_data)
for (i in cnames){
train_df[,i] = (train_df[,i] - mean(train_df[,i]))/sd(train_df[,i])
}
#test
numeric_index = sapply(test_df,is.numeric) #selecting only numeric
numeric_data = test_df[,numeric_index]
cnames = colnames(numeric_data)
for (i in cnames){
test_df[,i] = (test_df[,i] - mean(test_df[,i]))/sd(test_df[,i])
}
#---------------------------
## Naive Bayes
#---------------------------
NB_model = naiveBayes(Churn ~.,data=train_df)
NB_predictions = predict(NB_model, test_df[,-14],type = 'class')
conf_matrix_NB = table(test_df[,14],NB_predictions)
confusionMatrix(conf_matrix_NB)
sum(diag(conf_matrix_NB)/nrow(test_df))*100
#---------------------------
## Logistic regression
#---------------------------
logit = glm(Churn ~ ., train_df, family = 'binomial')
logit_predictions_prob = predict(logit, test_df[,-14], type = 'response')
logit_predictions = ifelse(logit_predictions_prob> 0.5, 1, 0)
ConfMatrix_logit = table(test_df[,14], logit_predictions)
confusionMatrix(ConfMatrix_logit)
sum(diag(ConfMatrix_logit)/nrow(test_df))*100
#---------------------------
## KNN
#---------------------------
knn_predictions = knn(train_df[,1:14],test_df[,1:14], train_df$Churn, k=3)
confMatrix_knn = table(knn_predictions, test_df$Churn)
confusionMatrix(confMatrix_knn)
sum(diag(confMatrix_knn)/nrow(test_df))*100
#------------------------------
#***********************************************************************************************
#As we have to calculate the churn score; therefore we will need an output in terms of probablity.
#Gaussian Naive Bayes and Logistic Regression gives probablity as output
#As accuracy of Logictic regression is higher as compared to Naive Bayes we will use Logistic Regression.
#************************************************************************************************
final_submission = subset(final_submission, select = -c(state,account.length,area.code,international.plan,
voice.mail.plan,number.vmail.messages,total.day.minutes,
total.day.calls,total.day.charge,total.eve.minutes,
total.eve.calls,total.eve.charge,total.night.minutes,total.night.calls,
total.night.charge,total.intl.minutes,total.intl.calls,total.intl.charge,
number.customer.service.calls,Churn))
final_submission$Churn.Score = logit_predictions_prob
write.csv(final_submission,'Churn_Score_r.csv')
rm(list=ls())
setwd("C:/Users/BATMAN/Desktop/1st project working copy")
getwd()
#------------------------------
library(corrplot)
library(DMwR)
library(e1071)
rm(list=ls())
setwd("C:/Users/BATMAN/Desktop/1st project working copy")
getwd()
#------------------------------
library(corrplot)
library(DMwR)
library(e1071)
library(caret)
library(class)
library(C50)
#------------------------------
# Load train data
train_df =  read.csv("C:/Users/BATMAN/Desktop/1st project working copy/Train_data.csv")
test_df =  read.csv("C:/Users/BATMAN/Desktop/1st project working copy/Test_data.csv")
final_submission = test_df
#------------------------------
#remove un-wanted variables
train_df <- subset(train_df, select = -c(state,area.code,phone.number))
test_df <- subset(test_df, select = -c(state,area.code,phone.number))
#------------------------------
#Replacing yes and no with 1 and 0/ true false with 1 and 0
#train
train_df$international.plan = as.numeric(as.factor(train_df$international.plan)==" yes",0,1)
train_df$voice.mail.plan = as.numeric(as.factor(train_df$voice.mail.plan)==" yes",0,1)
train_df$Churn = as.numeric(as.factor(train_df$Churn)==" True.",0,1)
#test
test_df$international.plan = as.numeric(as.factor(test_df$international.plan)==" yes",0,1)
test_df$voice.mail.plan = as.numeric(as.factor(test_df$voice.mail.plan)==" yes",0,1)
test_df$Churn = as.numeric(as.factor(test_df$Churn)==" True.",0,1)
#Converting into factor
train_df$Churn = as.factor(train_df$Churn)
train_df$international.plan = as.factor(train_df$international.plan)
train_df$voice.mail.plan = as.factor(train_df$voice.mail.plan)
test_df$Churn = as.factor(test_df$Churn)
test_df$international.plan = as.factor(test_df$international.plan)
test_df$voice.mail.plan = as.factor(test_df$voice.mail.plan)
#------------------------------
#Check missing vales
sum(is.na(train_df))
sum(is.na(test_df))
#------------------------------
## feature selection
#------------------------------
#correlation plot
w = train_df[,5:16]
x = cor(w)
corrplot(x, type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)
#From the above heat-map we can infer the following:
#- total day minutes & total day charge are highly +vely correlated.
#- total eve minutes & total eve charge are highly +vely correlated.
#- total night minutes & total night charge are highly +vely correlated.
#- total intl minutes & total intl charge are highly +vely correlated.
#Therefore we will drop the total day charge, total eve charge, total night charge i.e variables carrying redundant information
train_df <- subset(train_df, select = -c(total.day.minutes,total.eve.minutes,total.night.minutes,total.intl.minutes))
test_df <- subset(test_df, select = -c(total.day.minutes,total.eve.minutes,total.night.minutes,total.intl.minutes))
#------------------------------
## Outlier analysis using boxplot
#------------------------------
numeric_index = sapply(train_df,is.numeric) #selecting only numeric
#loop to plot box-plot for each variable
for(i in cnames)
numeric_data = train_df[,numeric_index]
cnames = colnames(numeric_data)
{
boxplot(train_df[i])
}
#loop to remove all outliers
#train
for(i in cnames)
{
val = train_df[,i][train_df[,i] %in% boxplot.stats(train_df[,i])$out]
train_df[,i][train_df[,i] %in% val] = NA
}
train_df = knnImputation(train_df, k=3)
cnames = colnames(numeric_data)
#------------------------------
## Outlier analysis using boxplot
#------------------------------
numeric_index = sapply(train_df,is.numeric) #selecting only numeric
numeric_data = train_df[,numeric_index]
cnames = colnames(numeric_data)
#loop to plot box-plot for each variable
for(i in cnames)
rm(list=ls())
setwd("C:/Users/BATMAN/Desktop/1st project working copy")
getwd()
#------------------------------
library(corrplot)
library(DMwR)
library(e1071)
library(caret)
library(class)
library(C50)
#------------------------------
# Load train data
train_df =  read.csv("C:/Users/BATMAN/Desktop/1st project working copy/Train_data.csv")
test_df =  read.csv("C:/Users/BATMAN/Desktop/1st project working copy/Test_data.csv")
final_submission = test_df
#------------------------------
#remove un-wanted variables
train_df <- subset(train_df, select = -c(state,area.code,phone.number))
test_df <- subset(test_df, select = -c(state,area.code,phone.number))
#------------------------------
#Replacing yes and no with 1 and 0/ true false with 1 and 0
#train
train_df$international.plan = as.numeric(as.factor(train_df$international.plan)==" yes",0,1)
train_df$voice.mail.plan = as.numeric(as.factor(train_df$voice.mail.plan)==" yes",0,1)
train_df$Churn = as.numeric(as.factor(train_df$Churn)==" True.",0,1)
#test
test_df$international.plan = as.numeric(as.factor(test_df$international.plan)==" yes",0,1)
test_df$voice.mail.plan = as.numeric(as.factor(test_df$voice.mail.plan)==" yes",0,1)
test_df$Churn = as.numeric(as.factor(test_df$Churn)==" True.",0,1)
#Converting into factor
train_df$Churn = as.factor(train_df$Churn)
train_df$international.plan = as.factor(train_df$international.plan)
train_df$voice.mail.plan = as.factor(train_df$voice.mail.plan)
test_df$Churn = as.factor(test_df$Churn)
test_df$international.plan = as.factor(test_df$international.plan)
test_df$voice.mail.plan = as.factor(test_df$voice.mail.plan)
#------------------------------
#Check missing vales
sum(is.na(train_df))
sum(is.na(test_df))
#------------------------------
## feature selection
#------------------------------
#correlation plot
w = train_df[,5:16]
x = cor(w)
corrplot(x, type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)
#From the above heat-map we can infer the following:
#- total day minutes & total day charge are highly +vely correlated.
#- total eve minutes & total eve charge are highly +vely correlated.
#- total night minutes & total night charge are highly +vely correlated.
#- total intl minutes & total intl charge are highly +vely correlated.
#Therefore we will drop the total day charge, total eve charge, total night charge i.e variables carrying redundant information
train_df <- subset(train_df, select = -c(total.day.minutes,total.eve.minutes,total.night.minutes,total.intl.minutes))
test_df <- subset(test_df, select = -c(total.day.minutes,total.eve.minutes,total.night.minutes,total.intl.minutes))
#------------------------------
## Outlier analysis using boxplot
#------------------------------
numeric_index = sapply(train_df,is.numeric) #selecting only numeric
numeric_data = train_df[,numeric_index]
cnames = colnames(numeric_data)
#loop to plot box-plot for each variable
for(i in cnames)
{
boxplot(train_df[i])
}
#loop to remove all outliers
#train
for(i in cnames)
{
val = train_df[,i][train_df[,i] %in% boxplot.stats(train_df[,i])$out]
train_df[,i][train_df[,i] %in% val] = NA
}
train_df = knnImputation(train_df, k=3)
numeric_index = sapply(test_df,is.numeric) #selecting only numeric
numeric_data = test_df[,numeric_index]
numeric_data <- subset(numeric_data, select = -c(number.vmail.messages))
cnames = colnames(numeric_data)
#test
for(i in cnames)
{
val = test_df[,i][test_df[,i] %in% boxplot.stats(test_df[,i])$out]
test_df[,i][test_df[,i] %in% val] = NA
}
test_df = knnImputation(test_df, k=3)
#------------------------------
## Feature scaling
#------------------------------
#standardization
#train
numeric_index = sapply(train_df,is.numeric) #selecting only numeric
numeric_data = train_df[,numeric_index]
cnames = colnames(numeric_data)
for (i in cnames){
train_df[,i] = (train_df[,i] - mean(train_df[,i]))/sd(train_df[,i])
}
#test
numeric_index = sapply(test_df,is.numeric) #selecting only numeric
numeric_data = test_df[,numeric_index]
cnames = colnames(numeric_data)
for (i in cnames){
test_df[,i] = (test_df[,i] - mean(test_df[,i]))/sd(test_df[,i])
}
#---------------------------
## Naive Bayes
#---------------------------
NB_model = naiveBayes(Churn ~.,data=train_df)
NB_predictions = predict(NB_model, test_df[,-14],type = 'class')
conf_matrix_NB = table(test_df[,14],NB_predictions)
confusionMatrix(conf_matrix_NB)
sum(diag(conf_matrix_NB)/nrow(test_df))*100
#---------------------------
## Logistic regression
#---------------------------
logit = glm(Churn ~ ., train_df, family = 'binomial')
logit_predictions_prob = predict(logit, test_df[,-14], type = 'response')
logit_predictions = ifelse(logit_predictions_prob> 0.5, 1, 0)
ConfMatrix_logit = table(test_df[,14], logit_predictions)
confusionMatrix(ConfMatrix_logit)
sum(diag(ConfMatrix_logit)/nrow(test_df))*100
#---------------------------
## KNN
#---------------------------
knn_predictions = knn(train_df[,1:14],test_df[,1:14], train_df$Churn, k=3)
confMatrix_knn = table(knn_predictions, test_df$Churn)
confusionMatrix(confMatrix_knn)
sum(diag(confMatrix_knn)/nrow(test_df))*100
#------------------------------
#***********************************************************************************************
#As we have to calculate the churn score; therefore we will need an output in terms of probablity.
#Gaussian Naive Bayes and Logistic Regression gives probablity as output
#As accuracy of Logictic regression is higher as compared to Naive Bayes we will use Logistic Regression.
#************************************************************************************************
final_submission = subset(final_submission, select = -c(state,account.length,area.code,international.plan,
voice.mail.plan,number.vmail.messages,total.day.minutes,
total.day.calls,total.day.charge,total.eve.minutes,
total.eve.calls,total.eve.charge,total.night.minutes,total.night.calls,
total.night.charge,total.intl.minutes,total.intl.calls,total.intl.charge,
number.customer.service.calls,Churn))
final_submission$Churn.Score = logit_predictions_prob
write.csv(final_submission,'Churn_Score_r.csv')
write.csv(final_submission,'Churn_Score_r.csv',index=FALSE)
write.csv(final_submission,'Churn_Score_r.csv',row.names=FALSE)
